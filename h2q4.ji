using LinearAlgebra
using Distributions
using Plots

function create_matrix_from_file(filename)  
# First pass: Determine the matrix dimensions
    max_doc_id, max_word_id = 0, 0
    open(filename, "r") do f
        for line in eachline(f)
            doc_id, word_id, _ = split(line)
            max_doc_id = max(max_doc_id, parse(Int, doc_id))
            max_word_id = max(max_word_id, parse(Int, word_id))
        end
    end

    # Initialize a matrix of zeros
    matrix = zeros(Int, max_doc_id, max_word_id)

    # Second pass: Fill the matrix directly
    open(filename, "r") do f
        for line in eachline(f)
            doc_id, word_id, freq = split(line)
            matrix[parse(Int, doc_id), parse(Int, word_id)] = parse(Int, freq)
        end
    end

    return matrix
end


function read_labels_from_file(filename)
    labels = Int[] 
    open(filename, "r") do f
        for line in eachline(f)
            push!(labels, parse(Int, strip(line))) 
        end
    end
    return labels
end


function prune_dataset(matrix, min_frequency)
    col_sums = sum(matrix, dims=1)
    mask = (col_sums .> min_frequency)[:]
    pruned_matrix = matrix[:, mask]
    return pruned_matrix, mask
end


function tfidf(X)
    N = size(X, 1)
    row_sums = sum(X, dims=2)
    tf = ifelse.(row_sums .== 0, 0, X ./ row_sums)
    df = sum(X .> 0, dims=1)
    idf = log.(N ./ (df .+ 1))
    tfidf_matrix = tf .* idf
    return tfidf_matrix
end


function orthogonal_iteration(phi, k)
    m, n = size(phi)
    theta = randn(m, k)
    
    for t in 1:1000
        theta_next = phi * phi' * theta
        Q, R = qr(theta_next)
        Q_new = Q[:, 1:k]
        
        if norm(Q_new - theta) < 1e-10
            println("Converged at iteration $t")
            break
        end

        theta = Q_new    
    end
    
    return theta
end


# initializing gmm
function initialize(X, k)
    n, d = size(X)
    means = X[sample(1:n, k, replace=false), :]
    covariances = [cov(X) for _ = 1:k]
    weights = ones(k) ./ k
    return means, covariances, weights
end


# defining the e-step of the EM algorithm
function e_step(X, means, covariances, weights, k)
    n = size(X, 1)
    responsibilities = zeros(n, k)

    for i = 1:n
        for j = 1:k
            responsibilities[i, j] = weights[j] * pdf(MvNormal(means[j, :], (0.5 * (covariances[j] + covariances[j]'))), X[i, :])
        end
    end
    
    row_sums = sum(responsibilities, dims=2)
    responsibilities .= responsibilities ./ row_sums
    
    return responsibilities
end

# defining the m-step of the EM algorithm
function m_step(X, responsibilities, k)
    n, d = size(X)
    nk = sum(responsibilities, dims=1)
    weights = nk ./ n
    
    means = (responsibilities' * X) ./ nk'
    
    covariances = []
    for j = 1:k
        X_centered = X .- means[j, :]'
        cov_matrix = (X_centered' * Diagonal(responsibilities[:, j]) * X_centered / nk[j]) + 1e-4 * I
        push!(covariances, cov_matrix)
    end
    
    return means, covariances, weights
end

# defining the EM algorithm
function gmm_em(X, k)
    means, covariances, weights = initialize(X, k)
    
    for t in 1:100
        responsibilities = e_step(X, means, covariances, weights, k)
        new_means, new_covariances, new_weights = m_step(X, responsibilities, k)
        
        if maximum(abs.(new_means - means)) < 1e-6
            break
        end
        means, covariances, weights = new_means, new_covariances, new_weights
    end
    
    return means, covariances, weights
end



X_train = create_matrix_from_file("./matlab/train.data")
X_train_pruned, mask = prune_dataset(X_train, 1000)
X_train_tfidf = tfidf(X_train_pruned)


klsi = 100
theta = orthogonal_iteration(X_train_tfidf', klsi)

phi = theta' * X_train_tfidf'

kgmm = 20
means, covariances, weights = gmm_em(phi, kgmm)


